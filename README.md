# Multi-Model Code Obfuscation Evaluation Study

_Generated by [Cline](https://cline.bot) with Claude Sonnet 4_

## Overview

This study evaluates how modern Large Language Models (LLMs) perform on code obfuscation analysis tasks, extending the research methodology from the paper ["How Does Naming Affect LLMs on Code Analysis Tasks?"](https://arxiv.org/abs/2307.12488) by testing newer models that didn't exist when the original research was conducted.

## Experimental Setup

### Models Tested

- **anthropic/claude-sonnet-4** - Latest Claude model (2024)
- **openai/gpt-4.1** - Advanced GPT-4 variant (2024)
- **google/gemini-2.5-pro-preview** - Gemini Pro preview (2024)

### Test Dataset

Four code pairs (original vs obfuscated) across multiple programming languages:

- `maxSubsequence.java` - Java algorithm with identifier renaming
- `SupperEggDrop.cpp` - C++ dynamic programming with complex obfuscation
- `carFleet.py` - Python algorithm with variable name obfuscation
- `DisconnectPath.cpp` - C++ graph algorithm with dead code injection

### Evaluation Criteria

Each model evaluated code pairs on:

- **Behavioral Equivalence**: Does obfuscated code preserve original behavior? (yes/no)
- **Security Preserved**: Are security properties maintained? (yes/no)
- **Obfuscation Type**: Classification of obfuscation technique used
- **Robustness Score**: Effectiveness of obfuscation (1-10 scale)
- **Recommendations**: Suggestions for improvement

### Technical Implementation

- **API**: OpenRouter with structured JSON schema outputs
- **Rate Limiting**: 2-second delays between requests, 3-second delays between models
- **Error Handling**: Comprehensive fallback parsing and error tracking
- **Output**: Real-time console display + detailed JSON report

## Experimental Issues

### Gemini Model Failures

Google's Gemini 2.5 Pro Preview experienced complete failure across all test cases:

- **Parse Errors**: Returned malformed JSON despite structured output schema
- **Invalid Responses**: Generated responses with syntax errors and incomplete data
- **Consistency Issues**: Unable to follow the specified JSON format
- **Result**: All Gemini evaluations marked as failed (N/A responses)

This suggests potential compatibility issues with OpenRouter's structured output implementation for Gemini models.

## Results

### Performance Summary

| Model           | Behavioral Equivalence | Security Preserved | Avg Robustness Score |
| --------------- | ---------------------- | ------------------ | -------------------- |
| Claude Sonnet 4 | 2/4 correct (50%)      | 2/4 correct (50%)  | 2.3/10               |
| GPT-4.1         | 3/4 correct (75%)      | 3/4 correct (75%)  | 2.8/10               |
| Gemini 2.5 Pro  | 0/4 (failed)           | 0/4 (failed)       | N/A                  |

### Detailed Analysis by File

#### 1. maxSubsequence.java (Simple Identifier Renaming)

- **Both models agreed**: Behavioral equivalence preserved, security maintained
- **Consensus**: Basic name obfuscation is ineffective (scores: 2-3/10)

#### 2. carFleet.py (Variable Name Obfuscation)

- **Both models agreed**: Behavior and security preserved
- **Consensus**: Minimal protection against analysis (scores: 2/10)

#### 3. DisconnectPath.cpp (Dead Code Injection)

- **Both models agreed**: Behavioral equivalence broken, security compromised
- **Consensus**: Obfuscation introduced compilation errors

#### 4. SupperEggDrop.cpp (Complex Obfuscation)

- **Critical disagreement**:
  - **Claude**: Identified compilation errors and undefined variables (no/no)
  - **GPT-4.1**: Missed critical flaws, deemed equivalent (yes/yes)
- **Analysis**: Claude demonstrated superior code analysis rigor

## Key Findings

### 1. Dramatic Improvement Over Original Research

The original paper (2020-2021) using CodeBERT/GraphCodeBERT found:

- Significant performance degradation with basic name obfuscation
- Heavy reliance on literal features vs logical understanding
- Models struggled with even simple identifier changes

**Modern LLMs (2024) show:**

- **Superior logical reasoning**: Correctly analyze through basic name obfuscation
- **Security awareness**: Understand security implications of code changes
- **Sophisticated analysis**: Provide detailed explanations and recommendations
- **Low obfuscation effectiveness**: Rate most techniques as 2-3/10 robustness

### 2. Model-Specific Behaviors

- **Claude Sonnet 4**: More conservative, better at detecting subtle errors
- **GPT-4.1**: More lenient, higher success rate but missed critical issues
- **Gemini 2.5 Pro**: Complete technical failure with structured outputs

### 3. Obfuscation Technique Effectiveness

- **Simple identifier renaming**: Completely ineffective against modern LLMs
- **Dead code injection**: Easily detected, often breaks functionality
- **Complex obfuscation**: May fool some models but creates maintenance issues

## Conclusions

### Evolution of LLM Capabilities

Modern LLMs demonstrate dramatically improved code analysis capabilities compared to models tested in the original 2020-2021 research. The ability to understand code logic independent of naming conventions has substantially advanced.

### Implications for Code Obfuscation

1. **Basic techniques are obsolete**: Simple identifier renaming provides no protection
2. **Quality matters**: Obfuscation that breaks functionality is counterproductive
3. **Sophistication required**: Effective obfuscation needs advanced techniques beyond naming
4. **Model differences**: Different LLMs show varying levels of analysis rigor

### Recommendations for Future Research

1. **Test advanced obfuscation**: Control flow flattening, opaque predicates, virtualization
2. **Expand model coverage**: Include more recent models and architectures
3. **Larger datasets**: Test with more diverse code samples and obfuscation techniques
4. **Longitudinal studies**: Track how model capabilities evolve over time

## Technical Notes

### Reproducibility

- All code and data available in this repository
- OpenRouter API key required for replication
- Ruby script with comprehensive error handling and logging

### Limitations

- Small dataset (4 code pairs)
- Gemini model failures limit cross-model comparison
- Focus on basic obfuscation techniques
- Single evaluation run per model

### Future Work

- Investigate Gemini compatibility issues
- Test with larger, more diverse datasets
- Evaluate advanced obfuscation techniques
- Compare with specialized code analysis tools

---

_This study demonstrates that while LLM capabilities have dramatically improved since 2020, this advancement makes traditional code obfuscation techniques even less effective against modern AI systems._
